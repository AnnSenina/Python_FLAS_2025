{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_CL_2025/blob/main/notebooks/Python_7_NLTK%2C_%D1%87%D0%B0%D1%81%D1%82%D0%BE%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%BF%D0%B8%D1%81%D0%BA%D0%B8%2C_n_%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK, частотные списки, n-граммы\n",
        "\n",
        "Подготовка текста для анализа\n",
        "\n",
        "<s>Ведь мы c вами все знаем, что на самом деле цифровой анализ текста - это и есть частотности слов</s> :)\n",
        "\n",
        "Используются материалы из тетрадок Д.Скоринкина, А. Хорошевой"
      ],
      "metadata": {
        "id": "xKMFDWff6OJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQfRhCxL3yA1"
      },
      "outputs": [],
      "source": [
        "# кое-что мы уже умеем, например:\n",
        "text = \" Давайте нормализуем этот текст!      \"\n",
        "\n",
        "print(text.lower().strip(\" !\").split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# или так:\n",
        "import re\n",
        "print(re.sub(r'\\W', ' ', text.lower()).split())"
      ],
      "metadata": {
        "id": "2IbRxBuAgW-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но пока мы получаем скорее псевдо-токены, например, слова с дефисом грубо делятся пополам\n",
        "\n",
        "В идеале нужно использовать какую-нибудь умную библиотеку, которая сама будет определять, нужно ли разрезать слово пополам или же оставить, как есть\n",
        "\n",
        "Классический вариант - nltk (хотя нарезать на токены будут уметь и другие библиотеки - mystem, spacy и т.д.)"
      ],
      "metadata": {
        "id": "K_cvrmu5hExg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Попробуем на реальном тексте\n",
        "По инерции от моих коллег берем в наследство пример с \"Преступлением и наказанием\"\n",
        "\n",
        "Проведем предобработку текста. Посмотрим на практике, на каком этапе нужна лемматизация."
      ],
      "metadata": {
        "id": "75EnFxyh9CNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Dostoevsky_PrestuplenieINakazanie.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "Rt48CpY_7G3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# нулевой шаг - переводим текст в нижний регистр\n",
        "text = text.lower()"
      ],
      "metadata": {
        "id": "tzuGUmxNB9w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Убедимся, что в тексте лежит то что мы ожидаем:"
      ],
      "metadata": {
        "id": "bViid1pcCC2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# начало романа:\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "1HDYpUUICBDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на все слова, в которых есть подстрока 'топор'\n",
        "for token in text.split():\n",
        "  if 'топор' in token:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "jSV0ykmGFkud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# чистим регуляркой\n",
        "# проверим, что проблема приклеившейся пунктуации ушла\n",
        "for word in re.sub(r'\\W', ' ', text.lower()).split():\n",
        "  if 'топор' in word:\n",
        "    print(word)\n",
        "# в принципе, работает!"
      ],
      "metadata": {
        "id": "SczKlOqaGEa9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Еще более умный способ:\n",
        "\n",
        "сегментируем текст готовым токенизатором — возьмем его из прекрасной библиотеки для обработки языка NLTK\n",
        "\n",
        "[Документация по NLTK](https://www.nltk.org/) и [книжка](https://www.nltk.org/book/)"
      ],
      "metadata": {
        "id": "HH5xf4oiGOHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "# %pip install nltk # в VS Code нужно устанавливать так\n",
        "\n",
        "# в Colab уже есть, в других средах - установите\n",
        "# в PyCharm мы можем устанавливать библиотеки в терминале (без знака !)"
      ],
      "metadata": {
        "id": "SCwW6hEo-8f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# библиотеки и импорты\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "# это подгрузка вспомогательных данных, которые нужны nltk для токенизации\n",
        "from nltk import download\n",
        "# download('punkt')\n",
        "# 'punkt' стал неактуальным в новой версии nltk (устаревший и небезопасный)\n",
        "# в новых версиях используется 'punkt_tab'\n",
        "# если у вас не работает с punct_tab - значит, версия nltk более старая (не страшно), раскоментируйте import punkt\n",
        "download('punkt_tab')\n",
        "\n",
        "# стоп-слова\n",
        "download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian')\n",
        "\n",
        "# стемминг\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "# разделение на предложения\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "gE1JTGwiGo9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала разбираемся с токенизатором на текстике попроще и покороче"
      ],
      "metadata": {
        "id": "cgHOBTTHkYkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
        "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
        "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\"\"\"\n",
        "# текст отсюда - https://habr.com/ru/post/582620/"
      ],
      "metadata": {
        "id": "Zjy_KO28T1Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(text_2))\n",
        "# wordpunct_tokenizer разбирает по регулярке - '\\w+|[^\\w\\s]+'\n",
        "# word_tokenize - тоже основан на регулярках, но более умных (учитывается последовательность некоторых символов, символы начала, конца слова и т.д).\n",
        "# для русского языка работает немного хуже, чем для английского"
      ],
      "metadata": {
        "id": "NPgNbDEgT177"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text_2))"
      ],
      "metadata": {
        "id": "i6dPXuIM0T4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# давайте быстро найдем различия:\n",
        "print(set(wordpunct_tokenize(text_2)) - set(word_tokenize(text_2)))\n",
        "print(set(word_tokenize(text_2)) - set(wordpunct_tokenize(text_2)))"
      ],
      "metadata": {
        "id": "nVfJ26Fh1QZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(wordpunct_tokenize(text_2)))"
      ],
      "metadata": {
        "id": "7_TjRBWv0qRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_tokenize(text_2)))"
      ],
      "metadata": {
        "id": "L13DAZ3P0rc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вернемся к Достоевскому, используем более умную токенизацию nltk\n",
        "text_list_nltk = word_tokenize(text)\n",
        "print(text_list_nltk[:30])"
      ],
      "metadata": {
        "id": "7M8E1sxJGwnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counter - встроенный счетчик частотностей в Python\n",
        "\n",
        "Мы его уже видели, в домашнем задании у вас ручной сбор словаря частот, а Counter быстро собирает этот словарь сам\n",
        "\n",
        "По сути, Counter заменяет ручную сборку словаря частотностей:\n",
        "\n",
        "```\n",
        "word_freqs = {}\n",
        "for i in text_list_nltk:\n",
        "  if i not in word_freqs:\n",
        "    word_freqs[i] = 1\n",
        "  else:\n",
        "    word_freqs[i] += 1\n",
        "    \n",
        "word_freqs = Counter(text_list_nltk) # то же самое\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPmJIpx9HnhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "pY3X7cnOHXq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counter - это специальный объект, который умеет легко считать повторяющиеся элементы в iterable\n",
        "# Например в строке:\n",
        "char_freqs = Counter('aaaaabbbbcccddefghik') # получим частотности всех элементов строки\n",
        "print(char_freqs)\n",
        "print(char_freqs.most_common(3)) # и топ-3 (5, 10 и т.д...) самых частотных элемента"
      ],
      "metadata": {
        "id": "4pYvF8oMHqx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs = Counter(text_list_nltk) # отправим в Counter\n",
        "word_freqs.most_common(10) # получим топ 10 \"слов\""
      ],
      "metadata": {
        "id": "sUnjcX-cIAX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk сохраняет пунктуацию как отдельный токен - избавимся от нее"
      ],
      "metadata": {
        "id": "BLgMdC_YlKcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# вспомним старые способы\n",
        "\n",
        "print(','.isalnum()) # True, если перед нами буква или цифра, НЕ знак препинания, НЕ пробел\n",
        "print('Санкт-Петербург'.isalnum()) # из-за дефиса тоже будет False\n",
        "\n",
        "# но можно проверять этим методом, что 'Санкт-Петербург' все-таки токен, а не просто знак пунктуации\n",
        "print('Санкт-Петербург'[0].isalnum())\n",
        "\n",
        "# 'Санкт-Петербург' -> False из-за дефиса\n",
        "# 'Санкт-Петербург'[0] -> 'С' -> True, т.к. начинается с буквы\n",
        "# ',' -> False\n",
        "# теперь все сработает"
      ],
      "metadata": {
        "id": "lvELGmdhIfFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_without_punkt = []\n",
        "for word in text_list_nltk:\n",
        "    if word[0].isalnum():\n",
        "        text_without_punkt.append(word)\n",
        "\n",
        "# на практике вы часто встретите списковые включения:\n",
        "# text_without_punkt = [word for word in text_list_nltk if word[0].isalnum()]\n",
        "# краткая форма записи цикла for"
      ],
      "metadata": {
        "id": "ZkmeiGNAI1kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_without_punkt[:30])"
      ],
      "metadata": {
        "id": "yQuKu10OJGz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(text_without_punkt).most_common(30)"
      ],
      "metadata": {
        "id": "0xhQQ7QwJRxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Чистим от стоп-слов"
      ],
      "metadata": {
        "id": "O0jIlYlbJhMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно загрузить свои стоп-слова и удалить их, но проще взять из NLTK: там есть наборы стоп-слов для разных языков\n"
      ],
      "metadata": {
        "id": "D_Anp2_nJkVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "print(stopwords.fileids()) # доступные языки"
      ],
      "metadata": {
        "id": "5GGXQfOOJtt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('russian')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "48lj7mrL6Zoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clean = []\n",
        "for word in text_without_punkt:\n",
        "  if word not in stop_words:\n",
        "    text_clean.append(word)\n",
        "\n",
        "# text_clean = [word for word in text_without_punkt if word not in stop_words]"
      ],
      "metadata": {
        "id": "M9EtTRs8J5Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_clean[:200])"
      ],
      "metadata": {
        "id": "eo7Ee7SEKSj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(text_clean).most_common(30)\n",
        "# хочется, конечно, удалить побольше стоп-слов\n",
        "# можно поискать другие библиотеки дополнительно или просто списки стоп-слов в интеренете\n",
        "# в рамках проектов не стесняйтесь чистить больше\n",
        "\n",
        "# кроме того, stop_words - список\n",
        "# вы можете его просто дополнить:\n",
        "# stop_words = stop_words + ['это', 'всё', 'очень', 'то', 'что', 'с'] # и т.д."
      ],
      "metadata": {
        "id": "ngpNxGtqKo_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Стемминг в Python\n",
        "\n",
        "Самый простой способ автоматический нормализации слов в языках с морфологией — стемминг. Стемминг — это очень грубое разбиение формы на предполагаемую основу и предполагаемую флексию.\n",
        "\n",
        "Программы-стеммеры умеют превращать:\n",
        "* \"Vyshka's students coded\" в \"Vyshka student code\"\n",
        "* 'Маша поехала за грибами' в 'Маш поехал за гриб'\n",
        "* 'Даня работает в Вышке' в \"Дан работа в Вышк\"\n",
        "\n",
        "Как можно догадаться из этих примеров, стемминг — не лучшее (и крайне непопулярное) решение для языков типа русского. Он лучше подходит для английского.\n",
        "\n",
        "В NLTK есть готовая реализация стеммера для русского языка. Давайте потестируем ее"
      ],
      "metadata": {
        "id": "Jzd9pOcYK4QJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Самый известный стеммер - стеммер Портера (или snowball стеммер).\n",
        "\n",
        "Подробнее про стеммер Портера можно почитать [вот тут](https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340)\n",
        "\n",
        "А совсем подробнее [вот тут](http://snowball.tartarus.org/algorithms/russian/stemmer.html)"
      ],
      "metadata": {
        "id": "eC3HE7FsVOca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"russian\") # в эту переменную мы сохраним уже готовый объект-стеммер для русского"
      ],
      "metadata": {
        "id": "OrZ65rHiLGu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# предположите, что выдаст стеммер?\n",
        "print(stemmer.stem('университетами'))\n",
        "print(stemmer.stem('мышам'))\n",
        "print(stemmer.stem('конями'))\n",
        "print(stemmer.stem('людей'))"
      ],
      "metadata": {
        "id": "deFj6YY_LRZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_stemmed = []\n",
        "for word in text_clean:\n",
        "  text_stemmed.append(stemmer.stem(word))\n",
        "\n",
        "# аналогично text_stemmed = [stemmer.stem(word) for word in text_clean]\n",
        "print(text_stemmed[:20])"
      ],
      "metadata": {
        "id": "k8qc06_DLTIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(text_stemmed).most_common(50)\n",
        "# этот вариант должен нас убедить, что без лемматизации все плохо :)"
      ],
      "metadata": {
        "id": "xr30gE4_P23o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сегментация на предложения\n",
        "\n",
        "NLTK также умеет разбивать текст на предложения"
      ],
      "metadata": {
        "id": "yVP3LUxI-sXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokenize(text)[:20]"
      ],
      "metadata": {
        "id": "ROOz9MV9_sqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание\n",
        "\n"
      ],
      "metadata": {
        "id": "V8HSb8QdPwZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соберем функцию, которая берет текст (строку), приводит к нижнему регистру, токенизирует, удаляет пунктуацию и стоп-слова (давайте не включать стемминг)"
      ],
      "metadata": {
        "id": "k5R3RVDg8BSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tokens(text):\n",
        "  # ваш код ниже\n",
        "\n",
        "\n",
        "  return text_clean"
      ],
      "metadata": {
        "id": "cZgyi7YT8As2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# не забудьте испытать функцию на Достоевском\n",
        "\n",
        "Counter(clean_tokens(text)).most_common(50)"
      ],
      "metadata": {
        "id": "yHlhch888eHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Решение ниже"
      ],
      "metadata": {
        "id": "n1fleWB6oXkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def clean_tokens(text):\n",
        "  text = text.lower()\n",
        "  text_list_nltk = word_tokenize(text)\n",
        "  text_without_punkt = [word for word in text_list_nltk if word[0].isalnum()]\n",
        "  text_clean = [word for word in text_without_punkt if word not in stop_words]\n",
        "  return text_clean"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WS7aojq0nson"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для большинства европейских языков вы найдете готовые наборы стоп-слов, пунктуация будет удаляться аналогично\n",
        "\n",
        "Но в китайском точно свои проблемы :)\n",
        "\n",
        "Давайте сегодня без токенизации китайского, бирманского и других подобных языков"
      ],
      "metadata": {
        "id": "szN0znFIocT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Добавим n-граммы и визуализации"
      ],
      "metadata": {
        "id": "Jajc4b-Qo15c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# еще импорты\n",
        "\n",
        "from nltk import Text as nltk_text\n",
        "\n",
        "# облако слов\n",
        "!pip install wordcloud # %pip install wordcloud # для VS Code\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install scikit-learn # %pip install scikit-learn # для VS Code\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "lnN3cCskpcSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Документация облака слов [здесь](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)"
      ],
      "metadata": {
        "id": "zeXGgy5IrAp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from wordcloud import WordCloud\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерируем облако слов\n",
        "wordcloud = WordCloud().generate_from_frequencies(Counter(clean_tokens(text)))\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # без подписей на осях\n",
        "plt.show() # показать изображение"
      ],
      "metadata": {
        "id": "8m1ay2JfpfAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# давайте отрисуем только слова, которые встречаются хотя бы 10 раз\n",
        "# добавляем аргумент margin\n",
        "\n",
        "wordcloud = WordCloud(margin = 10).generate_from_frequencies(Counter(clean_tokens(text)))\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # без подписей на осях\n",
        "plt.show() # показать изображение"
      ],
      "metadata": {
        "id": "LFs60Dwlq8e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width = 2000,\n",
        "                      height = 1500,\n",
        "                      background_color='white',\n",
        "                      margin = 10,\n",
        "                      colormap='viridis').generate_from_frequencies(Counter(clean_tokens(text)))\n",
        "plt.figure(figsize=(20, 15)) # Устанавливаем размер картинки\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # Без подписей на осях\n",
        "plt.show() # показать изображение\n"
      ],
      "metadata": {
        "id": "4VxbFMrTsU_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Больше цветовых схем [здесь](https://matplotlib.org/stable/users/explain/colors/colormaps.html)"
      ],
      "metadata": {
        "id": "47rIC-x0tUYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# еще один способ визуализации частот встроен в NLTK\n",
        "# from nltk import Text as nltk_text\n",
        "nltk_format = nltk_text(clean_tokens(text))\n",
        "nltk_format.dispersion_plot(['раскольников', 'соня', 'разумихин',\n",
        "                             'свидригайлов', 'дуня']);"
      ],
      "metadata": {
        "id": "d1e90flWtgEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gramms\n",
        "Текст можно разделить на n-граммы – устойчивые (частотные) сочетания по N слов:\n",
        "\n",
        "- nltk.bigrams() – сочетания по два слова\n",
        "- nltk.trigrams() – сочетания по три слова\n",
        "- nltk.ngrams(list, n) – сочетания по N слов"
      ],
      "metadata": {
        "id": "IBRCo-DWvSG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigramms = nltk.bigrams(clean_tokens(text))\n",
        "print(Counter(bigramms).most_common(20))\n",
        "\n",
        "trigramms = nltk.trigrams(clean_tokens(text))\n",
        "print(Counter(trigramms).most_common(10))\n",
        "\n",
        "ngramms = nltk.ngrams(clean_tokens(text), 4)\n",
        "print(Counter(ngramms).most_common(5))"
      ],
      "metadata": {
        "id": "OgwcE5K1ue_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Частотный анализ\n",
        "Многие компьтерные методы анализа текста основаны на статистике — в нашем случае это частотность символов / словоформ / лексем / биграмм / триграмм / частей речи и т.д., ее отношение к длине текста, средняя длина текстов и т.д.\n",
        "\n",
        "**Закон Ципфа**\n",
        "\n",
        "Закон Ципфа («ранг—частота») — эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (т.н. рангу этого слова). Например, второе по используемости слово встречается примерно в два раза реже, чем первое, третье — в три раза реже, чем первое, и т.д.\n",
        "\n",
        "Если закон Ципфа соблюдается — значит, перед нами нормальный текст на естественном языке. Если нет, то что-то с ним не так.\n",
        "\n",
        "**Закон Хипса**\n",
        "\n",
        "Закон Хипса — эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины.\n",
        "\n",
        "Чем больше коллекция текстов, тем меньше новых токенов появляется с её пополнением"
      ],
      "metadata": {
        "id": "rdIjfkZ6w-lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Способы считать частоту\n",
        "**Абсолютная частота слова**\n",
        "\n",
        "Количество употреблений слова в тексте. Она не всегда уместна.\n",
        "\n",
        "**Относительная частота слова**\n",
        "\n",
        "это отношение его абсолютной частоты к какой-нибудь другой величине, например, к длине текста или корпуса. Существуют разные способы подсчета относительной частоты.\n",
        "\n",
        "**IPM**\n",
        "\n",
        "Для сравнения частот в разных коллекциях текстов популярен ipm (items per million) - отношение абсолютной частоты какого-либо элемента к объему корпуса, умноженное на миллион.\n",
        "\n",
        "Метрика IPM позволяет сравнивать тексты через их характеристики. Например, \"Я\" заметно чаще встречается в корпусе любительской литературы, чем в корпусе художественных произведений из НКРЯ.\n",
        "\n",
        "Кстати, есть библиотеки с подсчетом ipm\n",
        "\n",
        "- [разные языки](https://pypi.org/project/wordfreq/)\n",
        "- отдельно [русский язык](https://pypi.org/project/ruword-frequency/)"
      ],
      "metadata": {
        "id": "_shMQd-dxQtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# можем быстро попробовать\n",
        "!pip install ruword-frequency\n",
        "from ruword_frequency import Frequency"
      ],
      "metadata": {
        "id": "bzPhcICBw1Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# как часто слово \"топор\" встречается в корпусе текстов на русском?\n",
        "freq = Frequency()\n",
        "freq.load()\n",
        "\n",
        "ipm = freq.ipm('топор')\n",
        "print(ipm)"
      ],
      "metadata": {
        "id": "GZWGCUnaxxCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# а как часто - в нашем тексте?\n",
        "print(clean_tokens(text).count(\"топор\") / len(clean_tokens(text)) * 1000000)"
      ],
      "metadata": {
        "id": "HiOtwf6Cx6bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "Tf-Idf - способ высоко оценить слова, которые одновременно\n",
        "\n",
        "- показательны в документе\n",
        "- не вездесущи в корпусе документов\n",
        "\n",
        "Наивная идея такая: давайте оценка слова будет\n",
        "\n",
        "- увеличиваться, если оно частотно в документе\n",
        "- уменьшаться, если оно встречается во многих документах\n",
        "\n",
        "В таком противостоянии победят те слова, которые выделяют документы из многих им подобных.\n",
        "\n",
        "[Гайд](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) из библиотеки sklearn"
      ],
      "metadata": {
        "id": "wcVJagoeyKU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(re.split(r'часть вторая|часть третья|часть четвертая|часть пятая|часть шестая|эпилог\\b', text))"
      ],
      "metadata": {
        "id": "V4kBtcboyKCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = re.split(r'часть вторая|часть третья|часть четвертая|часть пятая|часть шестая|эпилог\\b', text)"
      ],
      "metadata": {
        "id": "mMYQObEszgC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)"
      ],
      "metadata": {
        "id": "_mSSKuvhzl_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "tfidf.shape\n",
        "# предположите, что означают эти числа?"
      ],
      "metadata": {
        "id": "zKc4j-8rzpUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "import pandas as pd # знакомимся с pandas в следующий раз\n",
        "data = tfidf.todense().tolist()\n",
        "crime_and_punishment = pd.DataFrame(data, columns = words)\n",
        "\n",
        "crime_and_punishment"
      ],
      "metadata": {
        "id": "8EwwpSTUzwLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(zip(tfidf.toarray()[0], words), reverse=True)[:10]\n",
        "# топ-10 ключевых слов из первой части романа"
      ],
      "metadata": {
        "id": "DSoBLM930QaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# измените код из предыдущей строки, чтобы посмотреть на ключевые слова из эпилога\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LFNlJtFz0kVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "sorted(zip(tfidf.toarray()[6], words), reverse=True)[:10]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vrc9xZmq00pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание\n",
        "\n",
        "Используйте архив с текстами \"Девяти рассказов\" Селинджера, чтобы:\n",
        "- с помощью модуля shutil распакуйте архив, продите по файлам модулем os, чтобы считать все тексты в список (мини-корпус)\n",
        "- циклом for предобработайте каждый текст с помощью функции, сохраните списки токенов в новый список\n",
        "\n",
        "```[\n",
        "  [токен, токен2, токен3...],\n",
        "  [токен, токен2, токен3...],\n",
        "  ...\n",
        "]```\n",
        "\n",
        "- постройте 2 облака слов к любым текстам из девяти на ваш выбор\n",
        "- сравните топ-5 биграмм в этих текстах\n",
        "- сравните ключевые слова (tf-idf) этих текстов"
      ],
      "metadata": {
        "id": "ZDC85__i05fI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGGY9ov21mN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3eoR-aw1vei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asNDmsC81vY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IrdgRqNv1wDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwCtR6Eg1wZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import shutil, os\n",
        "shutil.unpack_archive('Salinger.zip')\n",
        "corpus = []\n",
        "for i in os.listdir('Salinger'):\n",
        "  with open('Salinger/' + i, 'r', encoding='utf-8') as f:\n",
        "    corpus.append(f.read())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VqtYm9S22Bge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "clean_corpus = []\n",
        "for i in corpus:\n",
        "  clean_corpus.append(clean_tokens(i))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X7ps0eDA2D13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                      margin = 10,\n",
        "                      colormap='viridis').generate_from_frequencies(Counter(clean_corpus[0]))\n",
        "\n",
        "                      # меняем индекс - и облако построится к другому тексту\n",
        "\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # Без подписей на осях\n",
        "plt.show() # показать изображение"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vEACP2J62DvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "bigramms = nltk.bigrams(clean_corpus[0]) # аналогично меняем индекс\n",
        "Counter(bigramms).most_common(5)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rHLPmFZJ2DlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "words = tfidf_vectorizer.get_feature_names_out()\n",
        "sorted(zip(tfidf.toarray()[0], words), reverse=True)[:10] # аналогично меняем индекс"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wht2Gwyi2Ddc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}