{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_FLAS_2025/blob/main/notebooks/%D0%9F%D0%B0%D1%80%D1%81%D0%B8%D0%BD%D0%B3_%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Парсинг_ практика"
      ],
      "metadata": {
        "id": "mYASBfjewihy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:18:38.198003Z",
          "start_time": "2020-12-19T08:18:38.186002Z"
        },
        "id": "XT5G41FYnr7a"
      },
      "outputs": [],
      "source": [
        "# импортируем модули в тетрадку\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlEvq5ofqGox"
      },
      "source": [
        "Гайды и туториалы\n",
        "\n",
        "- [документация requests и быстрый гайд](https://requests.readthedocs.io/en/master/user/quickstart/)\n",
        "\n",
        "\n",
        "- [документация Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "\n",
        "- [text-only](https://sjmulder.nl/en/textonly.html) страница содержит веб-сайты, чтобы легко начать парсить\n",
        "\n",
        "- [здесь](https://www.york.ac.uk/teaching/cws/wws/webpage1.html) можно почитать про структуру html подробнее\n",
        "\n",
        "\n",
        "- [здесь](https://www.w3schools.com/html/html_examples.asp) еще и потренироваться в режиме онлайн (с этой ссылки мы начали занятие)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задания\n",
        "\n",
        "1. Спарсите таблички со страницы https://en.wikipedia.org/wiki/List_of_nuclear_weapons_tests:\n",
        "\n",
        "- Worldwide nuclear testing totals by country\n",
        "- Worldwide nuclear test with a yield of 1.4 Mt TNT equivalent and more\n",
        "- Largest fission bomb tests\n",
        "\n",
        "Каждую табличку можно передать в датафрейм и сохранить в отдельный файл (удобнее спарсить ВСЕ с помощью pandas, потом выбрать 3 нужных вручную))"
      ],
      "metadata": {
        "id": "TuKaEA0iF4Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n"
      ],
      "metadata": {
        "id": "wZj4a6g0F9UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5k9p2myGJGTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIN_Z6ITJGLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Парсинг цитат с сайта:\n",
        "\n",
        "https://quotes.toscrape.com\n",
        "\n",
        "- Сгенерируйте ссылки на все 10 страниц цитат и сохраните все цитаты в текстовый файл\n",
        "\n",
        "- Затем попробуйте сохранить в датафрейм следующую информацию: автор, текст цитаты, тэги\n",
        "\n",
        "- соберите всю информацию об авторах: имя, когда человек родился, его биография, ссылка на его страницу"
      ],
      "metadata": {
        "id": "b_OBjvFPjwpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n"
      ],
      "metadata": {
        "id": "66l0Gyu7pRf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cQKAM-1pQxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KBGvK9GOpQqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Парсинг информации в датафрейм\n",
        "\n",
        "- соберите ссылки на все 50 страниц с сайта [Books to Scrape](https://books.toscrape.com/)\n",
        "- с каждой страницы всоберите все ссылки на отдельные книги (на этом шаге - **только** ссылки!)\n",
        "- затем пройдите по собранным ссылкам на отдельные книги соберите таблицу со следующими столбцами: ссылка, название книги, цена, текстовое описание\n",
        "\n",
        "**Важно!** текстовое описание должно вызвать проблему - можно сначала собрать без него, потом отловить через try / except и придумать, как обойти"
      ],
      "metadata": {
        "id": "PmFOGeNqudkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "# шаг 1 - ссылки на все страницы с книгами\n",
        "\n"
      ],
      "metadata": {
        "id": "pS8RuVlI3yAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5C6U6YzBvG44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YUVOp1PvGzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Все решения"
      ],
      "metadata": {
        "id": "fywT9ZqYxD9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1"
      ],
      "metadata": {
        "id": "_E0xQzOXxGlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "res = pd.read_html('https://en.wikipedia.org/wiki/List_of_nuclear_weapons_tests')\n",
        "print(len(res))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KbbL8oaXxHn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "df1 = pd.DataFrame(res[1])\n",
        "df2 = pd.DataFrame(res[2])\n",
        "df3 = pd.DataFrame(res[3])\n",
        "# табличек много, придется проверить первые несколько для поиска нужных"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JHd9O1DdxIAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2\n"
      ],
      "metadata": {
        "id": "IbyGE7b_xI2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# нагенерируем ссылок:\n",
        "all_links = []\n",
        "for i in range(1, 11):\n",
        "  all_links.append(f'https://quotes.toscrape.com/page/{i}')\n",
        "\n",
        "all_quotes = []\n",
        "for i in all_links:\n",
        "  page = requests.get(i)\n",
        "  soup = BeautifulSoup(page.text)\n",
        "  for qu in soup.find_all('span', {'class' : 'text', 'itemprop' : 'text'}):\n",
        "    all_quotes.append(qu.text.strip())\n",
        "print(*all_quotes[:5], sep='\\n')\n",
        "print('всего цитат:', len(all_quotes))\n",
        "\n",
        "with open('quotes.txt', 'w', encoding='utf-8') as f:\n",
        "  for i in all_quotes:\n",
        "    print(i, file = f)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iVoESBgZxKGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "texts = []\n",
        "authors = []\n",
        "tags = []\n",
        "for i in all_links:\n",
        "  page = requests.get(i)\n",
        "  soup = BeautifulSoup(page.text)\n",
        "  for qu in soup.find_all('span', {'class' : 'text', 'itemprop' : 'text'}):\n",
        "    texts.append(qu.text.strip())\n",
        "  for au in soup.find_all('small', {'class' : 'author', 'itemprop' : 'author'}):\n",
        "    authors.append(au.text.strip())\n",
        "  for t in soup.find_all('div', {'class' : 'tags'}):\n",
        "    tags.append(t.text.replace('Tags:', '').replace('\\n', ' ').strip())\n",
        "quotes = pd.DataFrame([texts, authors, tags])\n",
        "quotes = quotes.T\n",
        "quotes.columns = ['quote', 'author', 'tags']\n",
        "quotes"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FO99yLDuxKLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "authors_links = []\n",
        "for i in all_links:\n",
        "  page = requests.get(i)\n",
        "  soup = BeautifulSoup(page.text)\n",
        "  for link in soup.find_all('a'):\n",
        "    if 'author' in link.get('href') and 'https://quotes.toscrape.com' + link.get('href') not in authors_links:\n",
        "       authors_links.append('https://quotes.toscrape.com' + link.get('href'))\n",
        "len(authors_links) # авторы часто повторяются!"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2-ll3edDxKcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "all_data = []\n",
        "for i in authors_links:\n",
        "  try:\n",
        "    page = requests.get(i)\n",
        "    soup = BeautifulSoup(page.text)\n",
        "    name = soup.find('h3', {'class':\"author-title\"}).text.strip()\n",
        "    born = soup.find('span', {'class':\"author-born-date\"}).text.strip()\n",
        "    place = soup.find('span', {'class':\"author-born-location\"}).text.strip()\n",
        "    bio = soup.find('div', {'class':\"author-description\"}).text.strip()\n",
        "    author = [name, born, place, bio, i]\n",
        "    all_data.append(author)\n",
        "  except:\n",
        "    print(i, 'не сработало') # попала лишняя ссылка, но не мешает\n",
        "authors_df = pd.DataFrame(all_data)\n",
        "authors_df.columns = ['name', 'born', 'place', 'bio', 'link']\n",
        "authors_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iVLJs30R8UT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3"
      ],
      "metadata": {
        "id": "5-Wl-FwWxKlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# нагенерируем ссылок:\n",
        "all_links = []\n",
        "for i in range(1, 51):\n",
        "  all_links.append(f'https://books.toscrape.com/catalogue/page-{i}.html')\n",
        "\n",
        "book_links = []\n",
        "for i in all_links:\n",
        "  page = requests.get(i)\n",
        "  soup = BeautifulSoup(page.text)\n",
        "  for link in soup.find_all('a'):\n",
        "    if link.get('title') != None:\n",
        "      book_links.append('https://books.toscrape.com/catalogue/' + link.get('href'))\n",
        "print(len(book_links))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IrWhSQ62xNQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "books_data = []\n",
        "for i in tqdm(range(len(book_links))):\n",
        "  try:\n",
        "    page = requests.get(book_links[i])\n",
        "    soup = BeautifulSoup(page.text)\n",
        "    title = soup.find('h1').text.strip()\n",
        "    price = soup.find('p', {'class':\"price_color\"}).text.strip()\n",
        "    desc = soup.find('p', {'class':None}).text.strip() # оказалось, что не у всех есть описание!\n",
        "    books_data.append([title, price, desc, book_links[i]])\n",
        "  except:\n",
        "    print(book_links[i])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oYlKsmpfxNSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "books_data = []\n",
        "for i in tqdm(range(len(book_links))):\n",
        "  try:\n",
        "    page = requests.get(book_links[i])\n",
        "    soup = BeautifulSoup(page.text)\n",
        "    title = soup.find('h1').text.strip()\n",
        "    price = soup.find('p', {'class':\"price_color\"}).text.strip()\n",
        "    if soup.find('p', {'class':None}) != None: # исправляем\n",
        "      desc = soup.find('p', {'class':None}).text.strip()\n",
        "    else:\n",
        "      desc = None\n",
        "    books_data.append([title, price, desc, book_links[i]])\n",
        "  except:\n",
        "    print(book_links[i])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1fp6x5_eCCxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "books_df = pd.DataFrame(books_data)\n",
        "books_df.columns = ['title', 'price', 'description', 'link']\n",
        "books_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OTdlkT2QxNVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}